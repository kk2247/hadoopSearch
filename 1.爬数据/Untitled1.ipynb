{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_get():\n",
    "    #url = input(\"请输入要爬取的首页url:\")\n",
    "    url='https://blog.csdn.net/qq1332479771/article/details/62104624'\n",
    "    try:\n",
    "        headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'}\n",
    "        requests.get(url, headers=headers)\n",
    "        return url\n",
    "    except:\n",
    "        print(\"url无法连接\")\n",
    "    return url_get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiderpage(url,x):\n",
    "    kv = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER'}\n",
    "    r = requests.get(url, headers=kv)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    pagetext = r.text\n",
    "    pagelinks = re.findall(r'(?<=<a href=\\\").*?(?=\\\")|(?<=href=\\').*?(?=\\')', pagetext)\n",
    "    bsObj=BeautifulSoup(pagetext)\n",
    "    link=bsObj.body.a.text\n",
    "     \n",
    "    #page_list = response.xpath('//div[@class=\"l_post l_post_bright j_l_post clearfix  \"]')\n",
    "    \n",
    "    title=bsObj.body.h1.text\n",
    "    path='C:\\\\Users\\\\22478\\\\Desktop\\\\data\\\\'\n",
    "    fileName=path+str(x)+'.txt'\n",
    "    file = open(fileName, 'a', encoding='utf8')\n",
    "    file.write(f'{url}\\n')\n",
    "    file.write(f'{pagelinks}\\n')\n",
    "    file.write(f'{title}\\n')\n",
    "    file.write(f'{pagetext}\\n')\n",
    "    file.close()\n",
    "    return pagelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_filtrate(pagelinks):\n",
    "    # 去除不是以要爬取网站开头的url,如跳转的广告等\n",
    "    same_target_url = []\n",
    "    for l in pagelinks:\n",
    "        if re.findall(r'blog.csdn.net/\\w+/article/details/\\d+', l):\n",
    "            # 根据对网页的分析添加筛选条件,过滤掉系统推荐的博文链接\n",
    "            if re.findall(r'blockchain_lemon', l):\n",
    "                pass\n",
    "            # 过滤掉广告链接\n",
    "            elif re.findall(r'passport', l):\n",
    "                pass\n",
    "            else:\n",
    "                same_target_url.append(l)\n",
    "    # 去除重复url\n",
    "    unrepect_url = []\n",
    "    for l in same_target_url:\n",
    "        if l not in unrepect_url:\n",
    "            unrepect_url.append(l)\n",
    "    return unrepect_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linkQuence:\n",
    "    def __init__(self):\n",
    "        # 已访问的url集合\n",
    "        self.visited = []\n",
    "        # 待访问的url集合\n",
    "        self.unvisited = []\n",
    "\n",
    "    # 获取访问过的url队列\n",
    "    def getvisitedurl(self):\n",
    "        return self.visited\n",
    "\n",
    "    # 获取未访问的url队列\n",
    "    def getunvisitedurl(self):\n",
    "        return self.unvisited\n",
    "\n",
    "    # 添加url到访问过得队列中\n",
    "    def addvisitedurl(self, url):\n",
    "        return self.visited.append(url)\n",
    "\n",
    "    # 移除访问过得url\n",
    "    def removevisitedurl(self, url):\n",
    "        return self.visited.remove(url)\n",
    "\n",
    "    # 从未访问队列中取一个url\n",
    "    def unvisitedurldequence(self):\n",
    "        try:\n",
    "            return self.unvisited.pop()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # 添加url到未访问的队列中\n",
    "    def addunvisitedurl(self, url):\n",
    "        if url != \"\" and url not in self.visited and url not in self.unvisited:\n",
    "            return self.unvisited.insert(0, url)\n",
    "\n",
    "    # 获得已访问的url数目\n",
    "    def getvisitedurlount(self):\n",
    "        return len(self.visited)\n",
    "\n",
    "    # 获得未访问的url数目\n",
    "    def getunvistedurlcount(self):\n",
    "        return len(self.unvisited)\n",
    "\n",
    "    # 判断未访问的url队列是否为空\n",
    "    def unvisitedurlsempty(self):\n",
    "        return len(self.unvisited) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Spider():\n",
    "    def __init__(self, url):\n",
    "        self.linkQuence = linkQuence()  # 将队列引入本类\n",
    "        self.linkQuence.addunvisitedurl(url)  # 传入待爬取的url,即爬虫入口\n",
    "\n",
    "    def crawler(self,urlcount):\n",
    "        # 子页面过多,为测试方便加入循环控制子页面数量\n",
    "        x = 1\n",
    "        while x <= urlcount:\n",
    "            # 若子页面不是很多,可以直接使用队列中的未访问列表非空作为循环条件\n",
    "            # while not self.linkQuence.unvisitedurlsempty():\n",
    "            if x > 1:\n",
    "                print(f\"第{x-1}个url,开始爬\")\n",
    "            visitedurl = self.linkQuence.unvisitedurldequence()  # 从未访问列表中pop出一个url\n",
    "            if visitedurl is None or visitedurl == '':\n",
    "                continue\n",
    "            initial_links = spiderpage(visitedurl,x)  # 爬出该url页面中所有的链接\n",
    "            right_links = url_filtrate(initial_links)  # 筛选出合格的链接\n",
    "            self.linkQuence.addvisitedurl(visitedurl)  # 将该url放到访问过的url队列中\n",
    "            for link in right_links:  # 将筛选出的链接放到未访问队列中\n",
    "                self.linkQuence.addunvisitedurl(link)\n",
    "            x += 1\n",
    "        print(f\"终于爬完了,一共是{x-2}个url\")\n",
    "        return self.linkQuence.visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writetofile(list):\n",
    "    # 因为第一个爬取的页面为爬虫入口,非需要的博文网址,因此从[1]开始写入\n",
    "    x=1\n",
    "    for url in list[1:]:\n",
    "        # urls.txt用于保存博文标题和博文链接,文件夹demo创建好,或者加入os也行,反正很简单\n",
    "        file = open('E:/urls.txt', 'a', encoding='utf8')\n",
    "        file.write(f'{url}\\n')\n",
    "        x += 1\n",
    "    file.close()\n",
    "    print(f'写入已完成,总计{x-1}个子链接')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个url,开始爬\n",
      "第2个url,开始爬\n",
      "第3个url,开始爬\n",
      "第4个url,开始爬\n",
      "第5个url,开始爬\n",
      "第6个url,开始爬\n",
      "第7个url,开始爬\n",
      "第8个url,开始爬\n",
      "第9个url,开始爬\n",
      "第10个url,开始爬\n",
      "第11个url,开始爬\n",
      "第12个url,开始爬\n",
      "第13个url,开始爬\n",
      "第14个url,开始爬\n",
      "第15个url,开始爬\n",
      "第16个url,开始爬\n",
      "第17个url,开始爬\n",
      "第18个url,开始爬\n",
      "第19个url,开始爬\n",
      "第20个url,开始爬\n",
      "第21个url,开始爬\n",
      "第22个url,开始爬\n",
      "第23个url,开始爬\n",
      "第24个url,开始爬\n",
      "第25个url,开始爬\n",
      "第26个url,开始爬\n",
      "第27个url,开始爬\n",
      "第28个url,开始爬\n",
      "第29个url,开始爬\n",
      "第30个url,开始爬\n",
      "第31个url,开始爬\n",
      "第32个url,开始爬\n",
      "第33个url,开始爬\n",
      "第34个url,开始爬\n",
      "第35个url,开始爬\n",
      "第36个url,开始爬\n",
      "第37个url,开始爬\n",
      "第38个url,开始爬\n",
      "第39个url,开始爬\n",
      "第40个url,开始爬\n",
      "第41个url,开始爬\n",
      "第42个url,开始爬\n",
      "第43个url,开始爬\n",
      "第44个url,开始爬\n",
      "第45个url,开始爬\n",
      "第46个url,开始爬\n",
      "第47个url,开始爬\n",
      "第48个url,开始爬\n",
      "第49个url,开始爬\n",
      "第50个url,开始爬\n",
      "第51个url,开始爬\n",
      "第52个url,开始爬\n",
      "第53个url,开始爬\n",
      "第54个url,开始爬\n",
      "第55个url,开始爬\n",
      "第56个url,开始爬\n",
      "第57个url,开始爬\n",
      "第58个url,开始爬\n",
      "第59个url,开始爬\n",
      "第60个url,开始爬\n",
      "第61个url,开始爬\n",
      "第62个url,开始爬\n",
      "第63个url,开始爬\n",
      "第64个url,开始爬\n",
      "第65个url,开始爬\n",
      "第66个url,开始爬\n",
      "第67个url,开始爬\n",
      "第68个url,开始爬\n",
      "第69个url,开始爬\n",
      "第70个url,开始爬\n",
      "第71个url,开始爬\n",
      "第72个url,开始爬\n",
      "第73个url,开始爬\n",
      "第74个url,开始爬\n",
      "第75个url,开始爬\n",
      "第76个url,开始爬\n",
      "第77个url,开始爬\n",
      "第78个url,开始爬\n",
      "第79个url,开始爬\n",
      "第80个url,开始爬\n",
      "第81个url,开始爬\n",
      "第82个url,开始爬\n",
      "第83个url,开始爬\n",
      "第84个url,开始爬\n",
      "第85个url,开始爬\n",
      "第86个url,开始爬\n",
      "第87个url,开始爬\n",
      "第88个url,开始爬\n",
      "第89个url,开始爬\n",
      "第90个url,开始爬\n",
      "第91个url,开始爬\n",
      "第92个url,开始爬\n",
      "第93个url,开始爬\n",
      "第94个url,开始爬\n",
      "第95个url,开始爬\n",
      "第96个url,开始爬\n",
      "第97个url,开始爬\n",
      "第98个url,开始爬\n",
      "第99个url,开始爬\n",
      "第100个url,开始爬\n",
      "第101个url,开始爬\n",
      "第102个url,开始爬\n",
      "第103个url,开始爬\n",
      "第104个url,开始爬\n",
      "第105个url,开始爬\n",
      "第106个url,开始爬\n",
      "第107个url,开始爬\n",
      "第108个url,开始爬\n",
      "第109个url,开始爬\n",
      "第110个url,开始爬\n",
      "第111个url,开始爬\n",
      "第112个url,开始爬\n",
      "第113个url,开始爬\n",
      "第114个url,开始爬\n",
      "第115个url,开始爬\n",
      "第116个url,开始爬\n",
      "第117个url,开始爬\n",
      "第118个url,开始爬\n",
      "第119个url,开始爬\n",
      "第120个url,开始爬\n",
      "第121个url,开始爬\n",
      "第122个url,开始爬\n",
      "第123个url,开始爬\n",
      "第124个url,开始爬\n",
      "第125个url,开始爬\n",
      "第126个url,开始爬\n",
      "第127个url,开始爬\n",
      "第128个url,开始爬\n",
      "第129个url,开始爬\n",
      "第130个url,开始爬\n",
      "第131个url,开始爬\n",
      "第132个url,开始爬\n",
      "第133个url,开始爬\n",
      "第134个url,开始爬\n",
      "第135个url,开始爬\n",
      "第136个url,开始爬\n",
      "第137个url,开始爬\n",
      "第138个url,开始爬\n",
      "第139个url,开始爬\n",
      "第140个url,开始爬\n",
      "第141个url,开始爬\n",
      "第142个url,开始爬\n",
      "第143个url,开始爬\n",
      "第144个url,开始爬\n",
      "第145个url,开始爬\n",
      "第146个url,开始爬\n",
      "第147个url,开始爬\n",
      "第148个url,开始爬\n",
      "第149个url,开始爬\n",
      "第150个url,开始爬\n",
      "第151个url,开始爬\n",
      "第152个url,开始爬\n",
      "第153个url,开始爬\n",
      "第154个url,开始爬\n",
      "第155个url,开始爬\n",
      "第156个url,开始爬\n",
      "第157个url,开始爬\n",
      "第158个url,开始爬\n",
      "第159个url,开始爬\n",
      "第160个url,开始爬\n",
      "第161个url,开始爬\n",
      "第162个url,开始爬\n",
      "第163个url,开始爬\n",
      "第164个url,开始爬\n",
      "第165个url,开始爬\n",
      "第166个url,开始爬\n",
      "第167个url,开始爬\n",
      "第168个url,开始爬\n",
      "第169个url,开始爬\n",
      "第170个url,开始爬\n",
      "第171个url,开始爬\n",
      "第172个url,开始爬\n",
      "第173个url,开始爬\n",
      "第174个url,开始爬\n",
      "第175个url,开始爬\n",
      "第176个url,开始爬\n",
      "第177个url,开始爬\n",
      "第178个url,开始爬\n",
      "第179个url,开始爬\n",
      "第180个url,开始爬\n",
      "第181个url,开始爬\n",
      "第182个url,开始爬\n",
      "第183个url,开始爬\n",
      "第184个url,开始爬\n",
      "第185个url,开始爬\n",
      "第186个url,开始爬\n",
      "第187个url,开始爬\n",
      "第188个url,开始爬\n",
      "第189个url,开始爬\n",
      "第190个url,开始爬\n",
      "第191个url,开始爬\n",
      "第192个url,开始爬\n",
      "第193个url,开始爬\n",
      "第194个url,开始爬\n",
      "第195个url,开始爬\n",
      "第196个url,开始爬\n",
      "第197个url,开始爬\n",
      "第198个url,开始爬\n",
      "第199个url,开始爬\n",
      "第200个url,开始爬\n",
      "第201个url,开始爬\n",
      "第202个url,开始爬\n",
      "第203个url,开始爬\n",
      "第204个url,开始爬\n",
      "第205个url,开始爬\n",
      "第206个url,开始爬\n",
      "第207个url,开始爬\n",
      "第208个url,开始爬\n",
      "第209个url,开始爬\n",
      "第210个url,开始爬\n",
      "第211个url,开始爬\n",
      "第212个url,开始爬\n",
      "第213个url,开始爬\n",
      "第214个url,开始爬\n",
      "第215个url,开始爬\n",
      "第216个url,开始爬\n",
      "第217个url,开始爬\n",
      "第218个url,开始爬\n",
      "第219个url,开始爬\n",
      "第220个url,开始爬\n",
      "第221个url,开始爬\n",
      "第222个url,开始爬\n",
      "第223个url,开始爬\n",
      "第224个url,开始爬\n",
      "第225个url,开始爬\n",
      "第226个url,开始爬\n",
      "第227个url,开始爬\n",
      "第228个url,开始爬\n",
      "第229个url,开始爬\n",
      "第230个url,开始爬\n",
      "第231个url,开始爬\n",
      "第232个url,开始爬\n",
      "第233个url,开始爬\n",
      "第234个url,开始爬\n",
      "第235个url,开始爬\n",
      "第236个url,开始爬\n",
      "第237个url,开始爬\n",
      "第238个url,开始爬\n",
      "第239个url,开始爬\n",
      "第240个url,开始爬\n",
      "第241个url,开始爬\n",
      "第242个url,开始爬\n",
      "第243个url,开始爬\n",
      "第244个url,开始爬\n",
      "第245个url,开始爬\n",
      "第246个url,开始爬\n",
      "第247个url,开始爬\n",
      "第248个url,开始爬\n",
      "第249个url,开始爬\n",
      "第250个url,开始爬\n",
      "第251个url,开始爬\n",
      "第252个url,开始爬\n",
      "第253个url,开始爬\n",
      "第254个url,开始爬\n",
      "第255个url,开始爬\n",
      "第256个url,开始爬\n",
      "第257个url,开始爬\n",
      "第258个url,开始爬\n",
      "第259个url,开始爬\n",
      "第260个url,开始爬\n",
      "第261个url,开始爬\n",
      "第262个url,开始爬\n",
      "第263个url,开始爬\n",
      "第264个url,开始爬\n",
      "第265个url,开始爬\n",
      "第266个url,开始爬\n",
      "第267个url,开始爬\n",
      "第268个url,开始爬\n",
      "第269个url,开始爬\n",
      "第270个url,开始爬\n",
      "第271个url,开始爬\n",
      "第272个url,开始爬\n",
      "第273个url,开始爬\n",
      "第274个url,开始爬\n",
      "第275个url,开始爬\n",
      "第276个url,开始爬\n",
      "第277个url,开始爬\n",
      "第278个url,开始爬\n",
      "第279个url,开始爬\n",
      "第280个url,开始爬\n",
      "第281个url,开始爬\n",
      "第282个url,开始爬\n",
      "第283个url,开始爬\n",
      "第284个url,开始爬\n",
      "第285个url,开始爬\n",
      "第286个url,开始爬\n",
      "第287个url,开始爬\n",
      "第288个url,开始爬\n",
      "第289个url,开始爬\n",
      "第290个url,开始爬\n",
      "第291个url,开始爬\n",
      "第292个url,开始爬\n",
      "第293个url,开始爬\n",
      "第294个url,开始爬\n",
      "第295个url,开始爬\n",
      "第296个url,开始爬\n",
      "第297个url,开始爬\n",
      "第298个url,开始爬\n",
      "第299个url,开始爬\n",
      "第300个url,开始爬\n",
      "第301个url,开始爬\n",
      "第302个url,开始爬\n",
      "第303个url,开始爬\n",
      "第304个url,开始爬\n",
      "第305个url,开始爬\n",
      "第306个url,开始爬\n",
      "第307个url,开始爬\n",
      "第308个url,开始爬\n",
      "第309个url,开始爬\n",
      "第310个url,开始爬\n",
      "第311个url,开始爬\n",
      "第312个url,开始爬\n",
      "第313个url,开始爬\n",
      "第314个url,开始爬\n",
      "第315个url,开始爬\n",
      "第316个url,开始爬\n",
      "第317个url,开始爬\n",
      "第318个url,开始爬\n",
      "第319个url,开始爬\n",
      "第320个url,开始爬\n",
      "第321个url,开始爬\n",
      "第322个url,开始爬\n",
      "第323个url,开始爬\n",
      "第324个url,开始爬\n",
      "第325个url,开始爬\n",
      "第326个url,开始爬\n",
      "第327个url,开始爬\n",
      "第328个url,开始爬\n",
      "第329个url,开始爬\n",
      "第330个url,开始爬\n",
      "第331个url,开始爬\n",
      "第332个url,开始爬\n",
      "第333个url,开始爬\n",
      "第334个url,开始爬\n",
      "第335个url,开始爬\n",
      "第336个url,开始爬\n",
      "第337个url,开始爬\n",
      "第338个url,开始爬\n",
      "第339个url,开始爬\n",
      "第340个url,开始爬\n",
      "第341个url,开始爬\n",
      "第342个url,开始爬\n",
      "第343个url,开始爬\n",
      "第344个url,开始爬\n",
      "第345个url,开始爬\n",
      "第346个url,开始爬\n",
      "第347个url,开始爬\n",
      "第348个url,开始爬\n",
      "第349个url,开始爬\n",
      "第350个url,开始爬\n",
      "第351个url,开始爬\n",
      "第352个url,开始爬\n",
      "第353个url,开始爬\n",
      "第354个url,开始爬\n",
      "第355个url,开始爬\n",
      "第356个url,开始爬\n",
      "第357个url,开始爬\n",
      "第358个url,开始爬\n",
      "第359个url,开始爬\n",
      "第360个url,开始爬\n",
      "第361个url,开始爬\n",
      "第362个url,开始爬\n",
      "第363个url,开始爬\n",
      "第364个url,开始爬\n",
      "第365个url,开始爬\n",
      "第366个url,开始爬\n",
      "第367个url,开始爬\n",
      "第368个url,开始爬\n",
      "第369个url,开始爬\n",
      "第370个url,开始爬\n",
      "第371个url,开始爬\n",
      "第372个url,开始爬\n",
      "第373个url,开始爬\n",
      "第374个url,开始爬\n",
      "第375个url,开始爬\n",
      "第376个url,开始爬\n",
      "第377个url,开始爬\n",
      "第378个url,开始爬\n",
      "第379个url,开始爬\n",
      "第380个url,开始爬\n",
      "第381个url,开始爬\n",
      "第382个url,开始爬\n",
      "第383个url,开始爬\n",
      "第384个url,开始爬\n",
      "第385个url,开始爬\n",
      "第386个url,开始爬\n",
      "第387个url,开始爬\n",
      "第388个url,开始爬\n",
      "第389个url,开始爬\n",
      "第390个url,开始爬\n",
      "第391个url,开始爬\n",
      "第392个url,开始爬\n",
      "第393个url,开始爬\n",
      "第394个url,开始爬\n",
      "第395个url,开始爬\n",
      "第396个url,开始爬\n",
      "第397个url,开始爬\n",
      "第398个url,开始爬\n",
      "第399个url,开始爬\n",
      "第400个url,开始爬\n",
      "第401个url,开始爬\n",
      "第402个url,开始爬\n",
      "第403个url,开始爬\n",
      "第404个url,开始爬\n",
      "第405个url,开始爬\n",
      "第406个url,开始爬\n",
      "第407个url,开始爬\n",
      "第408个url,开始爬\n",
      "第409个url,开始爬\n",
      "第410个url,开始爬\n",
      "第411个url,开始爬\n",
      "第412个url,开始爬\n",
      "第413个url,开始爬\n",
      "第414个url,开始爬\n",
      "第415个url,开始爬\n",
      "第416个url,开始爬\n",
      "第417个url,开始爬\n",
      "第418个url,开始爬\n",
      "第419个url,开始爬\n",
      "第420个url,开始爬\n",
      "第421个url,开始爬\n",
      "第422个url,开始爬\n",
      "第423个url,开始爬\n",
      "第424个url,开始爬\n",
      "第425个url,开始爬\n",
      "第426个url,开始爬\n",
      "第427个url,开始爬\n",
      "第428个url,开始爬\n",
      "第429个url,开始爬\n",
      "第430个url,开始爬\n",
      "第431个url,开始爬\n",
      "第432个url,开始爬\n",
      "第433个url,开始爬\n",
      "第434个url,开始爬\n",
      "第435个url,开始爬\n",
      "第436个url,开始爬\n",
      "第437个url,开始爬\n",
      "第438个url,开始爬\n",
      "第439个url,开始爬\n",
      "第440个url,开始爬\n",
      "第441个url,开始爬\n",
      "第442个url,开始爬\n",
      "第443个url,开始爬\n",
      "第444个url,开始爬\n",
      "第445个url,开始爬\n",
      "第446个url,开始爬\n",
      "第447个url,开始爬\n",
      "第448个url,开始爬\n",
      "第449个url,开始爬\n",
      "第450个url,开始爬\n",
      "第451个url,开始爬\n",
      "第452个url,开始爬\n",
      "第453个url,开始爬\n",
      "第454个url,开始爬\n",
      "第455个url,开始爬\n",
      "第456个url,开始爬\n",
      "第457个url,开始爬\n",
      "第458个url,开始爬\n",
      "第459个url,开始爬\n",
      "第460个url,开始爬\n",
      "第461个url,开始爬\n",
      "第462个url,开始爬\n",
      "第463个url,开始爬\n",
      "第464个url,开始爬\n",
      "第465个url,开始爬\n",
      "第466个url,开始爬\n",
      "第467个url,开始爬\n",
      "第468个url,开始爬\n",
      "第469个url,开始爬\n",
      "第470个url,开始爬\n",
      "第471个url,开始爬\n",
      "第472个url,开始爬\n",
      "第473个url,开始爬\n",
      "第474个url,开始爬\n",
      "第475个url,开始爬\n",
      "第476个url,开始爬\n",
      "第477个url,开始爬\n",
      "第478个url,开始爬\n",
      "第479个url,开始爬\n",
      "第480个url,开始爬\n",
      "第481个url,开始爬\n",
      "第482个url,开始爬\n",
      "第483个url,开始爬\n",
      "第484个url,开始爬\n",
      "第485个url,开始爬\n",
      "第486个url,开始爬\n",
      "第487个url,开始爬\n",
      "第488个url,开始爬\n",
      "第489个url,开始爬\n",
      "第490个url,开始爬\n",
      "第491个url,开始爬\n",
      "第492个url,开始爬\n",
      "第493个url,开始爬\n",
      "第494个url,开始爬\n",
      "第495个url,开始爬\n",
      "第496个url,开始爬\n",
      "第497个url,开始爬\n",
      "第498个url,开始爬\n",
      "第499个url,开始爬\n",
      "终于爬完了,一共是499个url\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    url = url_get()\n",
    "    spider = Spider(url)\n",
    "    #传入要爬取的子链接数量100\n",
    "    urllist = spider.crawler(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
